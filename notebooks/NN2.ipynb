{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import load_posistions, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "1. Create dataset class\n",
    "2. Vectorize CSI to use it as input\n",
    "3. Create loader to iterate over the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSIDataset(Dataset):\n",
    "    \"\"\"CSI dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, positions_file, samples_dir, indices_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            positions_file (string): Path to the file containing the user positions.\n",
    "            samples_dir (string): Directory containing the samples.\n",
    "            indexes_file (string): Path to the file holding the indexes to be considered for the set\n",
    "        \"\"\"\n",
    "        self.user_positions = load_data(positions_file)\n",
    "        self.samples_dir = samples_dir\n",
    "        self.indices = load_data(indices_file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        \n",
    "        sample_filepath = os.path.join(self.samples_dir, 'channel_measurement_{:06d}.npy'.format(index))\n",
    "        sample = load_data(sample_filepath)\n",
    "                    \n",
    "        # Remove z coordinate from the positions\n",
    "        label = np.delete(self.user_positions[index], -1)\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize sample\n",
    "Each sample $\\hat h\\in\\mathbb{C}^{64\\times100}$ for the 64 antennas and 100 subcarriers. <br>\n",
    "Since each sample is a complex matrix, it is necessary to vectorize it to use it as input for the neural network. This is done first by concatenating the CSI values of each antenna into a vector:\n",
    "$$\n",
    "\\textbf{h}_k=\\begin{bmatrix}\n",
    "h_{k,1}\\\\\n",
    "h_{k,2}\\\\\n",
    "\\vdots\\\\\n",
    "h_{k,100}\n",
    "\\end{bmatrix} \\text{ for } k\\in\\{1,...,64\\}\n",
    "$$\n",
    "Then concatenating each $\\textbf{h}_k$ into a vector of size 6400:\n",
    "$$\n",
    "\\hat{\\textbf{h}}=\\begin{bmatrix}\n",
    "\\textbf{h}_1\\\\\n",
    "\\textbf{h}_2\\\\\n",
    "\\vdots\\\\\n",
    "\\textbf{h}_{64}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "h_{1,1}\\\\\n",
    "\\vdots\\\\\n",
    "h_{1,100}\\\\\n",
    "h_{2,1}\\\\\n",
    "\\vdots\\\\\n",
    "h_{2,100}\\\\\n",
    "\\vdots\\\\\n",
    "h_{64,1}\\\\\n",
    "\\vdots\\\\\n",
    "h_{64,100}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Finally, since Pytorch cannot handle complex numbers, it is necessary to tranform each CSI value $h$ into $h' =\\begin{bmatrix}real(h)\\\\ imag(h)\\end{bmatrix}$. <br>\n",
    "Thus, the final vector $\\hat{\\textbf h}$ is:\n",
    "$$\n",
    "\\hat{\\textbf h}=\n",
    "\\begin{bmatrix}\n",
    "real(h_{1,1})\\\\\n",
    "imag(h_{1,1})\\\\\n",
    "\\vdots\\\\\n",
    "real(h_{1,100})\\\\\n",
    "imag(h_{1,100})\\\\\n",
    "real(h_{2,1})\\\\\n",
    "imag(h_{2,1})\\\\\n",
    "\\vdots\\\\\n",
    "real(h_{2,100})\\\\\n",
    "imag(h_{2,100})\\\\\n",
    "\\vdots\\\\\n",
    "real(h_{64,1})\\\\\n",
    "imag(h_{64,1})\\\\\n",
    "\\vdots\\\\\n",
    "real(h_{64,100})\\\\\n",
    "imag(h_{64,100})\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "which is a $12800\\times1$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_samples(samples):\n",
    "    # concatenate into one 6400x1 vector\n",
    "    samples = torch.flatten(samples, start_dim=1)\n",
    "    # split complex values\n",
    "    samples = torch.stack((samples.real, samples.imag), -1) # vector is now 6400x2\n",
    "    # concatenate into one 12800x1 vector\n",
    "    samples = torch.flatten(samples, start_dim=1)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 252004\n",
      "Training set size: 201568\n",
      "Test set size: 50436\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSIDataset(\n",
    "    '/mnt/d/mathe/Documents/thesis/ultra_dense/DIS_lab_LoS/user_positions.npy',\n",
    "    '/mnt/d/mathe/Documents/thesis/ultra_dense/DIS_lab_LoS/samples',\n",
    "    '/mnt/d/mathe/Documents/thesis/ultra_dense/DIS_lab_LoS/train_indices.npy'\n",
    ")\n",
    "test_dataset = CSIDataset(\n",
    "    '/mnt/d/mathe/Documents/thesis/ultra_dense/DIS_lab_LoS/user_positions.npy',\n",
    "    '/mnt/d/mathe/Documents/thesis/ultra_dense/DIS_lab_LoS/samples',\n",
    "    '/mnt/d/mathe/Documents/thesis/ultra_dense/DIS_lab_LoS/test_indices.npy'\n",
    ")\n",
    "print(\n",
    "    f'Dataset size: {len(train_dataset) + len(test_dataset)}\\n'\n",
    "    f'Training set size: {len(train_dataset)}\\n'\n",
    "    f'Test set size: {len(test_dataset)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "Dataloader allows for some useful features when iterating over the dataset [\\[1\\]](#1), such as\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Loading the data in parallel using multiprocessing workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        self.fc1 = nn.Linear(12800, 16000)\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=4, stride=4)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=(4,2), stride=(4,2)) \n",
    "        self.conv3 = nn.Conv2d(8, 20, kernel_size=(1,5), stride=(1,5)) \n",
    "        self.fc2 = nn.Linear(500, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # First layer (fully connected)\n",
    "        x = self.fc1(x) # shape: 16000\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Reshape x to pass it through convolutional layers\n",
    "        x = x.reshape((-1, 1, 80, 200)) # shape: 1 x 80 x 200\n",
    "\n",
    "        x = self.conv1(x) # shape: 4 x 20 x 50 = 4000\n",
    "        # x = F.max_pool2d(x, kernel_size=(4,4)) # 4 x 20 x 50 = 4000\n",
    "        x = F.relu(x) # 4 x 20 x 50 = 4000\n",
    "        # print(f'Shape 1: {x.shape}')\n",
    "        \n",
    "        x = self.conv2(x) # shape: 8 x 5 x 25 = \n",
    "        # x = F.max_pool2d(x, kernel_size=2) # 8 x 10 x 25 = 1000\n",
    "        x = F.relu(x) # 8 x 10 x 25\n",
    "        # print(f'Shape 2: {x.shape}')\n",
    "        \n",
    "        x = self.conv3(x) # 20 x 5 x 5\n",
    "        # x = F.max_pool2d(x, kernel_size=(5,5)) # 16 x 1 x 5 = 80\n",
    "        x = F.relu(x) # 20 x 5 x 5 = 500\n",
    "        # print(f'Shape 3: {x.shape}')\n",
    "        \n",
    "        # Flatten to run through last layer\n",
    "        x = torch.flatten(x, 1) # shape: 500\n",
    "        # print(f'Shape 4: {x.shape}')\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Network(\n  (fc1): Linear(in_features=12800, out_features=16000, bias=True)\n  (conv1): Conv2d(1, 4, kernel_size=(4, 4), stride=(4, 4))\n  (conv2): Conv2d(4, 8, kernel_size=(4, 2), stride=(4, 2))\n  (conv3): Conv2d(8, 20, kernel_size=(1, 5), stride=(1, 5))\n  (fc2): Linear(in_features=500, out_features=2, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Choose GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100...  Training Loss: 15626.967...  Test Loss: 2887.307...  Elapsed time: 5551.155955076218\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:67] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 819200000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6871f437593b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Clip gradients to avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:67] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 819200000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Define the loss as MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "# Define optimizer to update weights (stochastic gradient descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 100\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "for e in range(epochs):\n",
    "    training_loss = 0\n",
    "    testing_loss = 0\n",
    "    for samples, labels in train_loader:\n",
    "        \n",
    "        samples, labels = samples.to(device), labels.to(device)\n",
    "        \n",
    "        # Vectorize the samples\n",
    "        samples = vectorize_samples(samples)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(samples.float())\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for samples, labels in test_loader:\n",
    "                samples, labels = samples.to(device), labels.to(device)\n",
    "        \n",
    "                # Vectorize the samples\n",
    "                samples = vectorize_samples(samples)\n",
    "\n",
    "                output = model(samples.float())\n",
    "                loss = criterion(output, labels.float())\n",
    "\n",
    "                testing_loss += loss.item()\n",
    "                \n",
    "        training_loss /= len(train_dataset)\n",
    "        testing_loss /= len(test_dataset)\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_loss)\n",
    "\n",
    "        now = time.time()\n",
    "        print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}... \".format(training_loss),\n",
    "              \"Test Loss: {:.3f}... \".format(testing_loss),\n",
    "              \"Elapsed time: {}\".format(now - start))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6871f437593b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Clip gradients to avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f28fc3722e0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses, label='Training loss')\n",
    "plt.plot(testing_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'simple_nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "<a id='1'>\\[1\\]</a> https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html (29.04.2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('thesis': conda)",
   "name": "python392jvsc74a57bd00d3f7e0036e7f4824600b1f90dd795bd69f11370b3401c9e9880074c085bf722"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}